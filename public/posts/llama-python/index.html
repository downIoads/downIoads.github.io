<!DOCTYPE html>
<html><head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Python script for Llama 2 conversations - Blog for Tech Enjoyers</title><link rel="icon" type="image/png" href=https://www.pngmart.com/files/23/Nerd-Emoji-PNG.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Simple Python script that let&#39;s you talk to your local Llama2 LLM using Windows." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="Python script for Llama 2 conversations" />
<meta property="og:description" content="Simple Python script that let&#39;s you talk to your local Llama2 LLM using Windows." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/posts/llama-python/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-26T18:00:23+02:00" />
<meta property="article:modified_time" content="2023-07-26T18:00:23+02:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Python script for Llama 2 conversations"/>
<meta name="twitter:description" content="Simple Python script that let&#39;s you talk to your local Llama2 LLM using Windows."/>
<script src="https://example.com/js/feather.min.js"></script>
	
	<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,500&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://example.com/css/main.af0932513936b0cde7d4a0d5917aa65438df9a57b01fb2769e2be4bdc492944f.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://example.com/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css"  disabled />
	
	
	
</head><body>
        <div class="content"><header>
	<div class="main">
		<a href="https://example.com/">Blog for Tech Enjoyers</a>
	</div>
	<nav>
		
		<a href="/posts">Posts</a>
		
		<a href="/about">About</a>
		
		<a href="/tags">Tags</a>
		
		| <a id="dark-mode-toggle" onclick="toggleTheme()" href=""></a>
		<script src="https://example.com/js/themetoggle.js"></script>
		
	</nav>
</header>
<main>

	<article>
		<div class="title">
			<h1 class="title">Python script for Llama 2 conversations</h1>
			<div class="meta">Posted on Jul 26, 2023</div>
		</div>
		

	<aside>
	<hr>
		Table of Contents:
        <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#python">Python</a></li>
    <li><a href="#impressions">Impressions</a></li>
    <li><a href="#outlook">Outlook</a></li>
  </ul>
</nav>
	<hr>
    </aside>


		<section class="body">
			<h2 id="introduction">Introduction</h2>
<p>I have played around a bit with the new Llama 2 LLM, more specifically with the 13B parameter Huggingface version that you can download <a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin">here</a>. In order to run it, check out <a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a>. It has precise setup instructions, so I will assume you get that running on your own. What I did not enjoy was having to type long commands into my Windows cmd every time, so I decided to write a short Python script that runs the process in the background and displays the output in the Python shell in real-time (well, almost). Windows paths are weird, especially when you try to put them in a Python string. Double-escaping \ did not seem to work, so I had to go with raw strings instead. Let&rsquo;s take a look at the script I am using:</p>
<h2 id="python">Python</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> signal
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> subprocess
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sys <span style="color:#f92672">import</span> exit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">signal_handler</span>(sig, frame):
</span></span><span style="display:flex;"><span>    exit(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">execute_command</span>(command):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># run command as global subprocess (that can be stopped at any time with signal_handler)</span>
</span></span><span style="display:flex;"><span>    process <span style="color:#f92672">=</span> subprocess<span style="color:#f92672">.</span>Popen(command, shell<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, stdout<span style="color:#f92672">=</span>subprocess<span style="color:#f92672">.</span>PIPE, stderr<span style="color:#f92672">=</span>subprocess<span style="color:#f92672">.</span>STDOUT, text<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># read from process and print real-time output to python shell</span>
</span></span><span style="display:flex;"><span>    first_line <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> process<span style="color:#f92672">.</span>stdout<span style="color:#f92672">.</span>readline()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> output <span style="color:#f92672">and</span> process<span style="color:#f92672">.</span>poll() <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># dont print the first line (it includes the prompt itself)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> first_line:
</span></span><span style="display:flex;"><span>            first_line <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(output<span style="color:#f92672">.</span>strip())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># wait until process finishes</span>
</span></span><span style="display:flex;"><span>    return_code <span style="color:#f92672">=</span> process<span style="color:#f92672">.</span>wait()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;--------------------------------------------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> return_code
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    signal<span style="color:#f92672">.</span>signal(signal<span style="color:#f92672">.</span>SIGINT, signal_handler)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        prompt <span style="color:#f92672">=</span> input(<span style="color:#e6db74">&#34;Prompt: &#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;--------------------------------------------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        command <span style="color:#f92672">=</span> (<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;&#34;C:\Users\myusername\Documents\llama.cpp\build\bin\Release\main.exe&#34; &#39;</span> <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;-m &#34;C:\Users\myusername\Documents\llama.cpp\build\bin\Release\models\llama-2-13b-chat.ggmlv3.q4_0.bin&#34; &#39;</span> <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;--color -p &#34;</span><span style="color:#e6db74">{</span>prompt<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34; 2&gt; $null&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        execute_command(command)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>main()
</span></span></code></pre></div><h2 id="impressions">Impressions</h2>
<p>Even though I compiled llama.cpp with CUDA support (CUDA is installed on my system too), running the model brings my CPU utilization to 50% (i9 12900k) while my GPU utilization is at 1% (RTX 4070). So I assume the specific model I have chosen either does not support CUDA or there is something wrong with my setup. Either way, the model output is approximately as fast as I am reading text, which is fine for now. Even though my Python code includes a signal handler that allows you to terminate the execution with Ctrl+C, I advise you against using this for now. The subprocess won&rsquo;t cleanly shut down and re-running the script can be problematic. I might update the script in the future to scan running processes and shut down any active instances of Llama&rsquo;s main.exe but for now I am happy with the script. Here is an example output:
<img src="/images/python_llama_example.png" alt="targets" title="Example prompt"></p>
<h2 id="outlook">Outlook</h2>
<p>I am glad we have half-decent language models that can run on consumer hardware. Considering how fast these models evolve and how the entry barriers for small models get lower, I can&rsquo;t wait to see what will be available in a few years!</p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/python">python</a></li>
					
					<li><a href="/tags/programming">programming</a></li>
					
					<li><a href="/tags/llama">llama</a></li>
					
					<li><a href="/tags/llm">llm</a></li>
					
					<li><a href="/tags/windows">windows</a></li>
					
					<li><a href="/tags/ai">ai</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
	

	
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2024  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


</div>
    </body>
</html>
