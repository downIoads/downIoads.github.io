<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Blog for Tech Enjoyers</title>
    <link>http://localhost:1313/tags/ai/</link>
    <description>Recent content in Ai on Blog for Tech Enjoyers</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Oct 2023 22:50:00 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Stable Diffusion as Discord Emote Generator</title>
      <link>http://localhost:1313/posts/stable-diffusion-gopher/</link>
      <pubDate>Fri, 20 Oct 2023 22:50:00 +0200</pubDate>
      <guid>http://localhost:1313/posts/stable-diffusion-gopher/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Stable Diffusion is an open-source deep learning text-to-image model that does not have any filters and works offline. There are a few controversies around it (e.g., it has been trained on copyrighted artwork but gives the user of Stable Diffusion the rights to all images it spits out, and it does not have filters and can be used to create all sorts of weird content). It works offline and poses a threat to existing businesses that sell AI-generated images (e.g., OpenAI&amp;rsquo;s closed-source DALLE-3). It is harder to use than, e.g., DALL-E because there are a lot of settings to adjust, which makes it highly customizable but also harder to use. I recently tried using Stable Diffusion again because &lt;a href=&#34;https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/&#34;&gt;NVIDIA published new Game Ready drivers for their RTX GPUs, which greatly increases Stable Diffusion&amp;rsquo;s performance&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Python script for Llama 2 conversations</title>
      <link>http://localhost:1313/posts/llama-python/</link>
      <pubDate>Wed, 26 Jul 2023 18:00:23 +0200</pubDate>
      <guid>http://localhost:1313/posts/llama-python/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;I have played around a bit with the new Llama 2 LLM, more specifically with the 13B parameter Huggingface version that you can download &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin&#34;&gt;here&lt;/a&gt;. In order to run it, check out &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;. It has precise setup instructions, so I will assume you get that running on your own. What I did not enjoy was having to type long commands into my Windows cmd every time, so I decided to write a short Python script that runs the process in the background and displays the output in the Python shell in real-time (well, almost). Windows paths are weird, especially when you try to put them in a Python string. Double-escaping \ did not seem to work, so I had to go with raw strings instead. Let&amp;rsquo;s take a look at the script I am using:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
